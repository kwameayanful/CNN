{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9602906,"sourceType":"datasetVersion","datasetId":5858703},{"sourceId":9603090,"sourceType":"datasetVersion","datasetId":5858828},{"sourceId":9604454,"sourceType":"datasetVersion","datasetId":5859832},{"sourceId":200491141,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"NAME: EMMANUEL KWAME AYANFUL","metadata":{}},{"cell_type":"markdown","source":"# 1. Training basic CNNs from scratch","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchsummary import summary\nfrom torch.optim import Adam, SGD\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.a. Basic CNN","metadata":{}},{"cell_type":"code","source":"# Define transform to normalize the train set\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Define transform to normalize the test set\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Gathering train and test data\ntrain_data = datasets.CIFAR10(\n    'data', train = True,\n    download = True, transform = transform_train\n)\ntest_data = datasets.CIFAR10(\n    'data', train = False,\n    download = True, transform = transform_test\n)\n\n# Split into train, validation and test\nnum_workers = 0 # Specify number of cpu cores to use\nbatch_size = 10\nvalid_size = 0.2 # Percentage of train data to be set aside for validation\ntrain_length = len(train_data)\nindices = list(range(train_length))\nsplit = int(np.floor(valid_size * train_length))\n\nnp.random.shuffle(indices) # Shuffle to introduce randomness\n\n# Get indices for train and validation data\ntrain_idx = indices[split : ]\nvalid_idx = indices[ : split]\n\n# Randomly sample train data using indices specified.\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalidation_sampler = SubsetRandomSampler(valid_idx)\n\n# Load train, validation and test data\ntrain_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, sampler = train_sampler\n)\nvalid_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, sampler = validation_sampler\n)\ntest_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, shuffle = True\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing a training batch\n\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',\n           'frog', 'horse', 'ship', 'truck']\n\n# print data informations\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\ndef imgShow(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n    img = img.numpy().transpose((1, 2, 0))\n    mean_ = np.array(mean)\n    std_ = np.array(std)\n    img = std_ * img + mean_\n    img = np.clip(img, 0, 1)\n    plt.imshow(img)\n    \n    \nfig = plt.figure(1, figsize=(10, 5))\nfor idx in range(batch_size):\n    ax = fig.add_subplot(\n        2, batch_size // 2,\n        idx + 1, xticks=[], yticks=[]\n    )\n    imgShow(images[idx])\n    ax.set_title(classes[labels[idx]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional Neural Network Architecture 1","metadata":{}},{"cell_type":"code","source":"class convNet1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=3, out_channels=6,\n            kernel_size=5, stride=1, padding=1\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=6, out_channels=16,\n            kernel_size=5, stride=1, padding=1\n        )\n        self.pool = nn.MaxPool2d(\n            kernel_size=2, stride=2\n        )\n        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x))) # (3x32x32) ---> (6x15x15)\n        x = self.pool(F.relu(self.conv2(x))) # (6x15x15) ---> (16x6x6)\n        x = torch.flatten(x, 1)              # (16x6x6) ---> 120               \n        x = F.relu(self.fc1(x))              # 120 ---> 84\n        x = F.relu(self.fc2(x))              # 84 ---> 10\n        x = self.fc3(x)\n        return x\n\nmodel1 = convNet1().cuda()\nsummary(model1, (3, 32, 32))   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainNetwork(model, lr, trainer, validator, optimizer='sgd', use_cuda=True, trainable_params=None):\n    # Ensure the model is on the correct device\n    if use_cuda and torch.cuda.is_available():\n        model = model.cuda()\n\n    # If no specific parameters are provided, train all parameters\n    if trainable_params is None:\n        trainable_params = model.parameters()\n    else:\n        trainable_params = [param for name, param in model.named_parameters() if name in trainable_params]\n\n    # Initialize the optimizer with the specified trainable parameters\n    if optimizer == 'adam':\n        optim = Adam(trainable_params, lr=lr)\n    elif optimizer == 'sgd':\n        optim = SGD(trainable_params, lr=lr, momentum=0.9)\n    \n    criterion = nn.CrossEntropyLoss()  # Loss function\n\n    epochs = 10  # Number of times to loop over data\n    track_loss = {'train': [], 'val': []}  # Dictionary to store loss history\n    val_loss_min = np.Inf\n\n    for epoch in range(epochs):\n        train_loss, valid_loss = 0, 0\n        total_train, correct_train = 0, 0\n        total_val, correct_val = 0, 0\n\n        # Training\n        model.train()  # Turn on dropout for training if specified in network\n        for images, labels in trainer:\n            if use_cuda and torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()\n            optim.zero_grad()  # Reset gradients of all optimized tensors\n            output = model(images)\n            loss = criterion(output, labels)  # Compute loss\n            loss.backward()  # Backpropagate errors\n            optim.step()  # Perform a single optimization step (parameter update)\n            train_loss += loss.item()\n            _, pred = torch.max(output, 1)\n            total_train += labels.size(0)\n            correct_train += (pred == labels).sum()\n\n        # Validating\n        model.eval()  # Turn off dropout for evaluation\n        for images, labels in validator:\n            if use_cuda and torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()\n            output = model(images)\n            loss = criterion(output, labels)  # Compute loss\n            valid_loss += loss.item()\n            _, pred = torch.max(output, 1)\n            total_val += labels.size(0)\n            correct_val += (pred == labels).sum()\n\n        # Compute and store loss in loss dictionary\n        train_loss /= len(trainer)\n        valid_loss /= len(validator)\n        track_loss['train'].append(train_loss)\n        track_loss['val'].append(valid_loss)\n\n        train_acc = 100 * (correct_train / total_train)\n        val_acc = 100 * (correct_val / total_val)\n\n        print(f\"[Epoch {epoch+1}]\\tTrain loss: {train_loss:.3f}\\tTrain Accuracy: {train_acc:.2f}%\\tValidation loss: {valid_loss:.3f}\\tVal Accuracy: {val_acc:.2f}%\")\n\n        if valid_loss <= val_loss_min:\n            val_loss_min = valid_loss\n            z = type(model).__name__\n            torch.save(model.state_dict(), z + '_model.pth')\n            print(\"Model state saved...\")\n\n    return track_loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss1 = trainNetwork(model1, 0.001, train_loader, valid_loader, use_cuda=True)\n\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss1['train'], label='Training loss')\nax.plot(loss1['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_1')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional Neural Network Architecture 2","metadata":{}},{"cell_type":"code","source":"class convNet2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=3, out_channels=32,\n            kernel_size=3, stride=1, padding=1\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=32, out_channels=64,\n            kernel_size=3, stride=1, padding=1\n        )\n        self.pool = nn.MaxPool2d(\n            kernel_size=2, stride=2\n        )\n        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \nmodel2 = convNet2().cuda()\nsummary(model2, (3, 32, 32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss2 = trainNetwork(model2, 0.001, train_loader, valid_loader, use_cuda=True)\n\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss2['train'], label='Training loss')\nax.plot(loss2['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_2')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional Neural Network Architecture 3","metadata":{}},{"cell_type":"code","source":"class convNet3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=3, out_channels=32,\n            kernel_size=3, stride=1, padding=1\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=32, out_channels=32,\n            kernel_size=3, stride=1, padding=1\n        )\n        self.pool = nn.MaxPool2d(\n            kernel_size=2, stride=2\n        )\n        self.dropout1 = nn.Dropout(0.3)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(32*16*16, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = self.dropout1(F.relu(self.conv1(x))) # (3x32x32) ---> (32x16x16)\n        x = self.pool(F.relu(self.conv2(x)))     # (32x16x16) ---> (32x8x8)\n        x = torch.flatten(x, 1)                  # (16x4x4) ---> 256\n        x = self.dropout2(F.relu(self.fc1(x)))   # 8192 ---> 512\n        x = self.fc2(x)                          # 128 ---> 10\n        return x\n    \nmodel3 = convNet3().cuda()\nsummary(model3, (3, 32, 32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss3 = trainNetwork(model3, 0.001, train_loader, valid_loader, use_cuda=True)\n\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss3['train'], label='Training loss')\nax.plot(loss3['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_1')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Performamce of Models on test set","metadata":{}},{"cell_type":"code","source":"def test(model, use_cuda=False):\n    test_loss = 0\n    total, correct = 0, 0\n    criterion = nn.CrossEntropyLoss() # loss function\n\n    model.eval() # test the model with dropout layers off\n    for images,labels in test_loader:\n        if use_cuda and torch.cuda.is_available():\n            images, labels = images.cuda(), labels.cuda()\n        output = model(images)\n        loss = criterion(output, labels)\n        test_loss += loss.item()\n        _, pred = torch.max(output, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum()\n\n    test_loss = test_loss / len(test_loader)\n    acc = 100 * (correct / total)\n    print(f'For {type(model).__name__}:')\n    print(f\"Test Loss: {test_loss:.3f}\")\n    print(f\"Test Accuracy: {acc:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_models = [model1, model2, model3]\nfor model in cnn_models:\n    test(model, use_cuda=True)\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Classification with transfer learning","metadata":{}},{"cell_type":"code","source":"# Define transform to normalize the train set\ntransform_train = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n# Define transform to normalize the test set\ntransform_test = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n# Gathering train and test data\ntrain_data = datasets.CIFAR10(\n    'data', train = True,\n    download = True, transform = transform_train\n)\ntest_data = datasets.CIFAR10(\n    'data', train = False,\n    download = True, transform = transform_test\n)\n\n# Split into train, validation and test\nnum_workers = 0 # Specify number of cpu cores to use\nbatch_size = 10\nvalid_size = 0.2 # Percentage of train data to be set aside for validation\ntrain_length = len(train_data)\nindices = list(range(train_length))\nsplit = int(np.floor(valid_size * train_length))\n\nnp.random.shuffle(indices) # Shuffle to introduce randomness\n\n# Get indices for train and validation data\ntrain_idx = indices[split : ]\nvalid_idx = indices[ : split]\n\n# Randomly sample train data using indices specified.\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalidation_sampler = SubsetRandomSampler(valid_idx)\n\n# Load train, validation and test data\ntrain_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, sampler = train_sampler\n)\nvalid_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, sampler = validation_sampler\n)\ntest_loader = DataLoader(\n    train_data, num_workers = num_workers,\n    batch_size = batch_size, shuffle = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print data informations\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nfig = plt.figure(1, figsize=(10, 5))\nfor idx in range(batch_size):\n    ax = fig.add_subplot(\n        2, batch_size // 2,\n        idx + 1, xticks=[], yticks=[]\n    )\n    imgShow(images[idx])\n    ax.set_title(classes[labels[idx]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Efficient Net B0","metadata":{}},{"cell_type":"code","source":"efficientnetB0 = models.efficientnet_b0(weights='IMAGENET1K_V1')\n#Freeze the convolutional base (except the final layers)\nfor param in efficientnetB0.parameters():\n    param.requires_grad = False\n    \nnum_ftrs = efficientnetB0.classifier[1].in_features\nefficientnetB0.classifier[1] = nn.Linear(num_ftrs, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in efficientnetB0.named_parameters():\n    if param.requires_grad:\n        print(f\"Parameter {name} requires gradients.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainable_params=[name for name, param in efficientnetB0.named_parameters() if param.requires_grad]\nloss_e_netb0 = trainNetwork(efficientnetB0, 0.001, train_loader, valid_loader, use_cuda=True)\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss_e_netb0['train'], label='Training loss')\nax.plot(loss_e_netb0['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_EfficientNet_B0')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Squeezenet1_0","metadata":{}},{"cell_type":"code","source":"squeezenet1_0 = models.squeezenet1_0(weights='IMAGENET1K_V1')\n\n#Freeze the convolutional base (except the final layers)\nfor param in squeezenet1_0.parameters():\n    param.requires_grad = False\n\n# The original squeezenet model has 1,000 outputs so we change that to 10\nsqueezenet1_0.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\nsqueezenet1_0.num_classes = 10\n\nfor name, param in squeezenet1_0.named_parameters():\n    if param.requires_grad:\n        print(f\"Parameter {name} requires gradients.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainable_params=[name for name, param in squeezenet1_0.named_parameters() if param.requires_grad]\nloss_squeezenet1_0 = trainNetwork(squeezenet1_0, 0.001, train_loader, valid_loader, use_cuda=True)\n\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss_squeezenet1_0['train'], label='Training loss')\nax.plot(loss_squeezenet1_0['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_SqueezeNet1_0')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shuffle Net","metadata":{}},{"cell_type":"code","source":"shufflenet = models.shufflenet_v2_x0_5(weights='IMAGENET1K_V1')\n\n#Freeze the convolutional base (except the final layers)\nfor param in shufflenet.parameters():\n    param.requires_grad = False\n\n# Modify the classifier to output 10 classes for CIFAR-10\nnum_ftrs = shufflenet.fc.in_features\nshufflenet.fc = nn.Linear(num_ftrs, 10)\n\nfor name, param in shufflenet.named_parameters():\n    if param.requires_grad:\n        print(f\"Parameter {name} requires gradients.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable_params=[name for name, param in shufflenet.named_parameters() if param.requires_grad]\nloss_shufflenet = trainNetwork(shufflenet, 0.001, train_loader, valid_loader, use_cuda=True,\n                           trainable_params = trainable_params)\n\n# View loss graph\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loss_shufflenet['train'], label='Training loss')\nax.plot(loss_shufflenet['val'], label='Validation loss')\nax.set_title('Network Performance on CIFAR10-Model_Shufflenet')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_models = [efficientnetB0.cuda(), squeezenet1_0.cuda(), shufflenet.cuda()]\nfor model in cnn_models:\n    test(model, use_cuda=True)\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Saliency Maps","metadata":{}},{"cell_type":"code","source":"# Load Pretrained Model\nmodel = models.efficientnet_b0(weights='IMAGENET1K_V1')\nmodel.eval()  # Set the model to evaluation mode\n\n# Preprocess the image\ndef preprocess_image(image_path):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    return image_tensor\n\n# Function to slide an occluding square over the image\ndef occlude_and_predict(model, image_tensor, occlusion_size=15, stride=5):\n    _, _, h, w = image_tensor.size()\n    output_map = np.zeros((h, w))  # To store probabilities for each occluded region\n\n    original_output = model(image_tensor)  # Original model output\n    original_probabilities = F.softmax(original_output, dim=1).detach().numpy()\n\n    # Get the correct class (highest probability)\n    predicted_class = np.argmax(original_probabilities)\n\n    for y in range(0, h - occlusion_size + 1, stride):\n        for x in range(0, w - occlusion_size + 1, stride):\n            # Create a copy of the image\n            occluded_image = image_tensor.clone()\n            \n            # Zero-out a portion of the image (the occlusion square)\n            occluded_image[:, :, y:y + occlusion_size, x:x + occlusion_size] = 0.0\n            \n            # Forward pass with the occluded image\n            output = model(occluded_image)\n            probabilities = F.softmax(output, dim=1).detach().numpy()\n\n            # Store the probability of the correct class for the occluded region\n            output_map[y:y + occlusion_size, x:x + occlusion_size] = probabilities[0, predicted_class]\n    \n    return output_map, predicted_class\n\n# Step 4: Generate the occlusion-based saliency map\nocclusion_size = 15  # Size of the occluding square\nstride = 8  # Stride of the occluding square\noutput_map, predicted_class = occlude_and_predict(model, image_tensor, occlusion_size, stride)\n\n# Step 5: Visualize the saliency map\ndef show_saliency_map(output_map, image_path):\n    image = Image.open(image_path).convert('RGB')\n    plt.figure(figsize=(10, 5))\n    \n    # Plot original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n    plt.title(\"Original Image\")\n    \n    # Plot saliency map\n    plt.subplot(1, 2, 2)\n    plt.imshow(output_map, cmap='jet', interpolation='nearest')\n    plt.title(\"Occlusion-Based Saliency Map\")\n    plt.colorbar()\n    plt.show()\n\nimage_path = '/kaggle/input/dogggg/n02088466_bloodhound.JPEG'\nimage_tensor = preprocess_image(image_path)\n\n# Show saliency map for the test image\nshow_saliency_map(output_map, image_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}